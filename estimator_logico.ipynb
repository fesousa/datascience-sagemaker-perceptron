{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df404785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import TrainingInput\n",
    "import numpy as np\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = 'datascience-sagemaker-fernandosousa'\n",
    "prefix = 'perceptron/op_logicos'\n",
    "\n",
    "instance_type = 'local' # trocar para um inst√¢ncia, por exemplo ml.m5.large, para treinar e implantar no sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edd45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar dados de treinamento\n",
    "dados = [\n",
    "      [0, 0, 0],\n",
    "      [0, 1, 0],\n",
    "      [1, 0, 0],\n",
    "      [1, 1, 1]\n",
    "    ]\n",
    "x = np.array(dados)\n",
    "# save to csv file\n",
    "np.savetxt('data/train.csv', x, delimiter=',')\n",
    "\n",
    "# enviar para s3\n",
    "sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "# definir caminhos para sagemaker copiar os dados\n",
    "train_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train.csv\"), content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    \"s3://{}/{}/{}\".format(bucket, prefix, \"train.csv\"), content_type=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9af435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='perceptron.py',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters={\n",
    "                        'error':0.1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61450d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating p47qigspg4-algo-1-ygxy5 ... \n",
      "Creating p47qigspg4-algo-1-ygxy5 ... done\n",
      "Attaching to p47qigspg4-algo-1-ygxy5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,682 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,685 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,697 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,700 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,933 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,950 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,972 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:06,986 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Training Env:\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"training\": \"/opt/ml/input/data/training\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"validation\": \"/opt/ml/input/data/validation\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     },\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"current_host\": \"algo-1-ygxy5\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"algo-1-ygxy5\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     ],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"error\": 0.1\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     },\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"training\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m             \"TrainingInputMode\": \"File\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m             \"ContentType\": \"csv\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         },\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"validation\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m             \"TrainingInputMode\": \"File\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m             \"ContentType\": \"csv\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         }\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     },\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"job_name\": \"pytorch-training-2022-03-02-01-06-03-734\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"master_hostname\": \"algo-1-ygxy5\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-559966685160/pytorch-training-2022-03-02-01-06-03-734/source/sourcedir.tar.gz\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"module_name\": \"perceptron\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"current_host\": \"algo-1-ygxy5\",\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m             \"algo-1-ygxy5\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         ]\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     },\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m     \"user_entry_point\": \"perceptron.py\"\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m }\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Environment variables:\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_HOSTS=[\"algo-1-ygxy5\"]\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_HPS={\"error\":0.1}\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_USER_ENTRY_POINT=perceptron.py\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ygxy5\",\"hosts\":[\"algo-1-ygxy5\"]}\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_CHANNELS=[\"training\",\"validation\"]\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_CURRENT_HOST=algo-1-ygxy5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_MODULE_NAME=perceptron\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-559966685160/pytorch-training-2022-03-02-01-06-03-734/source/sourcedir.tar.gz\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1-ygxy5\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-ygxy5\"],\"hyperparameters\":{\"error\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"csv\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-03-02-01-06-03-734\",\"log_level\":20,\"master_hostname\":\"algo-1-ygxy5\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-559966685160/pytorch-training-2022-03-02-01-06-03-734/source/sourcedir.tar.gz\",\"module_name\":\"perceptron\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ygxy5\",\"hosts\":[\"algo-1-ygxy5\"]},\"user_entry_point\":\"perceptron.py\"}\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_USER_ARGS=[\"--error\",\"0.1\"]\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m SM_HP_ERROR=0.1\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m /opt/conda/bin/python3.6 perceptron.py --error 0.1\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Dados local /opt/ml/input/data/training\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Dados local tensor([[1., 1., 1., 1.],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [0., 0., 1., 1.],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [0., 1., 0., 1.]])\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Dados local /opt/ml/input/data/validation\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Dados local tensor([[1., 1., 1., 1.],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [0., 0., 1., 1.],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [0., 1., 0., 1.]])\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m [2022-03-02 01:06:07.862 algo-1-ygxy5:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m [2022-03-02 01:06:08.090 algo-1-ygxy5:27 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.75\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m MSE 0.0\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Pesos Final: Parameter containing:\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m tensor([[-0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0100]], requires_grad=True)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m Salvando modelo. /opt/ml/model\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m STATE: OrderedDict([('module.weight', tensor([[-0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0100]]))])\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.75\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.25\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.5\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:MSE 0.0\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:Pesos Final: Parameter containing:\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m tensor([[-0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0100]], requires_grad=True)\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:Salvando modelo. /opt/ml/model\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m INFO:__main__:STATE: OrderedDict([('module.weight', tensor([[-0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0200],\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m         [ 0.0100]]))])\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m \n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 |\u001b[0m 2022-03-02 01:06:08,254 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mp47qigspg4-algo-1-ygxy5 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": train_input, \"validation\": validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ff7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to m15eooxqzu-algo-1-5q3qp\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,402 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Torchserve version: 0.3.0\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Current directory: /\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Number of CPUs: 2\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Max heap size: 988 M\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Log dir: /logs\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Netty threads: 0\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Default workers per model: 2\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Enable metrics API: true\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,444 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,457 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag f66e80e36fa9408694572313ba5b32cc\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,469 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,484 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,648 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,663 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]38\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,663 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,664 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,682 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,729 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,729 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,741 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,745 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,769 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,771 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]39\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,771 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,772 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,774 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:12,786 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m Model server started.\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,413 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,417 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:8.04294204711914|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,417 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:100.07106018066406|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,418 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:92.6|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,418 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2634.55859375|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,418 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1055.578125|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,421 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:33.2|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,815 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 931\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,816 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:1338|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,821 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:138|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,858 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 982\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,860 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:1380|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183173\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:13,860 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:91|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:14,239 [INFO ] pool-1-thread-3 ACCESS_LOG - /172.18.0.1:53360 \"GET /ping HTTP/1.1\" 200 16\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:14,244 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type, serializer=CSVSerializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00df1255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,093 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 11\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,094 [INFO ] W-9001-model_1 ACCESS_LOG - /172.18.0.1:53372 \"POST /invocations HTTP/1.1\" 200 14\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,094 [INFO ] W-9001-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,094 [INFO ] W-9001-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:10.35|#ModelName:model,Level:Model|#hostname:ca352d4924c7,requestID:749881d9-b316-4304-b4ae-118776a4c2d5,timestamp:1646183180\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,095 [INFO ] W-9001-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:20,096 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:59,802 [INFO ] pool-1-thread-3 ACCESS_LOG - /172.18.0.1:56600 \"GET /ping HTTP/1.1\" 200 0\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:06:59,803 [INFO ] pool-1-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:ca352d4924c7,timestamp:null\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,330 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,332 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:8.038818359375|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,332 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:100.0751838684082|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,333 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:92.6|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,334 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2441.91015625|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,335 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1248.65625|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n",
      "\u001b[36mm15eooxqzu-algo-1-5q3qp |\u001b[0m 2022-03-02 01:07:13,336 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:38.1|#Level:Host|#hostname:ca352d4924c7,timestamp:1646183233\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(np.array([1,1,0]))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
